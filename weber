import requests
from bs4 import BeautifulSoup
from datetime import datetime
import pandas as pd

# === Step 1: Take user input ===
start_date_str = input("Enter start date (yyyy-mm-dd): ").strip()

try:
    start_date = datetime.strptime(start_date_str, "%Y-%m-%d")
except ValueError:
    print("âŒ Invalid format. Use yyyy-mm-dd (e.g., 2025-06-01).")
    exit()

today = datetime.today()
delta_days = (today - start_date).days

if delta_days < 0:
    print("âŒ Start date cannot be in the future.")
    exit()

# === Step 2: Construct the URL ===
url = f"https://www.legislation.gov.au/search/registrationdate(today-{delta_days},today)/collection(act)/sort(registeredat%20desc)"
print(f"\nğŸ” Fetching data from:\n{url}\n")

# === Step 3: Make request and parse HTML ===
headers = {
    "User-Agent": "Mozilla/5.0"
}
response = requests.get(url, headers=headers)

if response.status_code != 200:
    print("âŒ Failed to fetch the page. Status:", response.status_code)
    exit()

soup = BeautifulSoup(response.text, "html.parser")

# === Step 4: Extract articles ===
results = []

for item in soup.select("frl-grid-cell-title-name-in-force .title-name a"):
    title = item.text.strip()
    relative_link = item.get("href", "").strip()
    full_url = "https://www.legislation.gov.au" + relative_link if relative_link else "N/A"

    # Registered date is found in nearby sibling
    reg_info = item.find_parent().find_next_sibling("div")
    reg_date = "N/A"

    if reg_info and "Registered:" in reg_info.text:
        text = reg_info.get_text(strip=True)
        if "Registered:" in text:
            reg_date = text.split("Registered:")[-1].strip()

    results.append({
        "Title": title,
        "Registered Date": reg_date,
        "URL": full_url
    })

# === Step 5: Save to DataFrame and CSV ===
df = pd.DataFrame(results)
df.to_csv("acts_scraped.csv", index=False)

print("âœ… Done! Saved to 'acts_scraped.csv'")
print(df.head())

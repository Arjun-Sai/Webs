import requests
from bs4 import BeautifulSoup
from datetime import datetime
import pandas as pd
import time
import io
import PyPDF2

def extract_pdf_content(pdf_url, headers):
    try:
        pdf_response = requests.get(pdf_url, headers=headers, timeout=30)
        if pdf_response.status_code != 200:
            return None
        pdf_file = io.BytesIO(pdf_response.content)
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        text_content = []
        for page in pdf_reader.pages:
            page_text = page.extract_text()
            if page_text:
                text_content.append(page_text)
        return " ".join(" ".join(text_content).split())
    except Exception as e:
        return None

def extract_content_with_fallback(base_pdf_url, headers):
    """
    Try extracting content from base URL first.
    If it fails, then try multivolume extraction using /1, /2, ... until a volume is not found.
    """
    print(f"Trying base PDF first...")
    base_content = extract_pdf_content(base_pdf_url, headers)

    if base_content:
        print(f"Base PDF extracted successfully.")
        return base_content

    # Base failed, try volumes
    print(f"Base failed. Trying multivolume PDFs...")
    content_parts = []
    volume = 1

    while True:
        volume_url = f"{base_pdf_url}/{volume}"
        try:
            response = requests.get(volume_url, headers=headers, timeout=15, stream=True)
            print("h2")
            print(volume_url)
            if response.status_code != 200:
                print("h1")
                print(response.status_code)
                print(f"Volume {volume} not found, stopping.")
                break
            response.close()

            print(f"Found Volume {volume}, extracting...")
            vol_text = extract_pdf_content(volume_url, headers)
            if vol_text:
                content_parts.append(f"=== VOLUME {volume} ===\n\n{vol_text}")
                print(f"Volume {volume} added.")
            else:
                print(f"Volume {volume} extraction failed.")

        except requests.exceptions.RequestException as e:
            print(f"Error with Volume {volume}: {e}")
            break

        volume += 1
        time.sleep(0.5)

    if content_parts:
        return "\n\n" + ("\n\n" + "="*60 + "\n\n").join(content_parts)
    else:
        return "No content extracted"

# === Step 1: User input
start_date_str = input("Enter start date (yyyy-mm-dd): ").strip()
try:
    start_date = datetime.strptime(start_date_str, "%Y-%m-%d")
except ValueError:
    print("Invalid format.")
    exit()

today = datetime.today()
delta_days = (today - start_date).days

if delta_days < 0:
    print("Start date is in the future.")
    exit()

# === Step 2: Construct search URL
url = f"https://www.legislation.gov.au/search/registrationdate(today-{delta_days},today)/collection(act)/sort(registeredat%20desc)"
print(f"\n🔍 Searching:\n{url}\n")

headers = {
    "User-Agent": "Mozilla/5.0"
}

response = requests.get(url, headers=headers)
if response.status_code != 200:
    print("Failed to fetch page.")
    exit()

soup = BeautifulSoup(response.text, "html.parser")

# === Step 4: Process articles
results = []
items = soup.select("frl-grid-cell-title-name-in-force .title-name a")
print(f"Found {len(items)} acts.\n")

for idx, item in enumerate(items, 1):
    print(f"Processing {idx}/{len(items)}")
    title = item.text.strip()
    relative_link = item.get("href", "").strip()
    base_url = "https://www.legislation.gov.au"
    full_url = base_url + relative_link if relative_link else "N/A"

    # Registered date
    reg_info = item.find_parent().find_next_sibling("div")
    reg_date = "N/A"
    if reg_info and "Registered:" in reg_info.text:
        reg_date = reg_info.get_text(strip=True).split("Registered:")[-1].strip()

    effective_date = "N/A"
    content = "N/A"
    download_link = "N/A"

    act_id = relative_link.split("/")[1] if relative_link else None
    if full_url != "N/A":
        detail_response = requests.get(full_url, headers=headers)
        if detail_response.status_code == 200:
            detail_soup = BeautifulSoup(detail_response.text, "html.parser")
            eff_span = detail_soup.select_one("span.date-effective-start")
            if eff_span:
                effective_date = eff_span.text.strip()
                try:
                    eff_dt_obj = datetime.strptime(effective_date, "%d %B %Y")
                    eff_dt_str = eff_dt_obj.strftime("%Y-%m-%d")
                    pdf_url = f"{base_url}/{act_id}/{eff_dt_str}/{eff_dt_str}/text/original/pdf"
                    download_link = pdf_url
                    print(f"Attempting content extraction from {pdf_url}")
                    content = extract_content_with_fallback(pdf_url, headers)

                    if len(content) > 10000:
                        content = content[:10000] + "... [TRUNCATED]"

                except Exception as e:
                    print(f"Date parsing failed: {e}")

        time.sleep(1)

    results.append({
        "Title": title,
        "Registered Date": reg_date,
        "URL": full_url,
        "Content": content
    })

pd.set_option("display.max_colwidth", None)

# === Step 5: Save
df = pd.DataFrame(results)
df.to_excel("acts_scraped_with_multivolumes.xlsx", index=False)

print("\nDone. Data saved to 'acts_scraped_with_multivolumes.csv'")
print(df[["Title", "Registered Date", "URL", "Content"]].head())

import requests
from bs4 import BeautifulSoup
from datetime import datetime
import pandas as pd
import time
import io
import PyPDF2
# Alternative: import pdfplumber

def extract_pdf_content(pdf_url, headers):
    """
    Download PDF from URL and extract text content
    """
    try:
        # Download PDF content
        pdf_response = requests.get(pdf_url, headers=headers, timeout=30)
        if pdf_response.status_code != 200:
            return "‚ùå Failed to download PDF"

        # Create a file-like object from the PDF content
        pdf_file = io.BytesIO(pdf_response.content)

        # Extract text using PyPDF2
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        text_content = ""

        for page_num in range(len(pdf_reader.pages)):
            page = pdf_reader.pages[page_num]
            text_content += page.extract_text() + "\n"

        # Clean up the text (remove excessive whitespace)
        text_content = " ".join(text_content.split())

        return text_content if text_content.strip() else "‚ùå No text extracted"

    except Exception as e:
        return f"‚ùå Error extracting PDF: {str(e)}"

def extract_all_volumes_content(base_pdf_url, headers):
    """
    Extract content from all volumes of a multi-volume document
    Checks for volumes by appending /1, /2, /3, etc. to the base URL
    """
    all_content = []
    volume_num = 1
    max_volumes = 50  # Safety limit to prevent infinite loops
    consecutive_failures = 0
    max_consecutive_failures = 3  # Stop after 3 consecutive failures

    print(f"    Checking for multiple volumes...")

    while volume_num <= max_volumes and consecutive_failures < max_consecutive_failures:
        # Construct volume-specific URL
        volume_url = f"{base_pdf_url}/{volume_num}"

        try:
            # First, check if the volume exists (HEAD request is faster)
            head_response = requests.head(volume_url, headers=headers, timeout=15)

            if head_response.status_code == 200:
                print(f"    üìÑ Found Volume {volume_num}, extracting content...")

                # Extract content from this volume
                volume_content = extract_pdf_content(volume_url, headers)

                if volume_content and not volume_content.startswith("‚ùå"):
                    all_content.append(f"=== VOLUME {volume_num} ===\n\n{volume_content}")
                    consecutive_failures = 0  # Reset failure counter
                    print(f"    ‚úÖ Volume {volume_num} extracted successfully")
                else:
                    print(f"    ‚ö†Ô∏è Volume {volume_num} found but content extraction failed")
                    consecutive_failures += 1

            elif head_response.status_code == 404:
                print(f"    üì≠ Volume {volume_num} not found (404)")
                consecutive_failures += 1
            else:
                print(f"    ‚ùå Volume {volume_num} returned status {head_response.status_code}")
                consecutive_failures += 1

        except requests.exceptions.RequestException as e:
            print(f"    ‚ùå Error checking Volume {volume_num}: {str(e)}")
            consecutive_failures += 1

        volume_num += 1
        time.sleep(0.5)  # Small delay between volume checks

    # If no volumes found, try the base URL without volume number
    if not all_content:
        print(f"    üìÑ No volumes found, trying base PDF URL...")
        base_content = extract_pdf_content(base_pdf_url, headers)
        if base_content and not base_content.startswith("‚ùå"):
            all_content.append(base_content)
            print(f"    ‚úÖ Base PDF extracted successfully")
        else:
            print(f"    ‚ùå Base PDF extraction failed: {base_content}")

    # Combine all volume content
    if all_content:
        combined_content = "\n\n" + "="*50 + "\n\n".join(all_content)
        total_volumes = len(all_content)
        print(f"    ‚úÖ Successfully extracted {total_volumes} volume(s)")
        return combined_content
    else:
        return "‚ùå No content extracted from any volume"

# Alternative function using pdfplumber (often better for complex PDFs)
def extract_pdf_content_pdfplumber(pdf_url, headers):
    """
    Download PDF from URL and extract text content using pdfplumber
    """
    try:
        import pdfplumber

        # Download PDF content
        pdf_response = requests.get(pdf_url, headers=headers, timeout=30)
        if pdf_response.status_code != 200:
            return "‚ùå Failed to download PDF"

        # Create a file-like object from the PDF content
        pdf_file = io.BytesIO(pdf_response.content)

        # Extract text using pdfplumber
        text_content = ""
        with pdfplumber.open(pdf_file) as pdf:
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    text_content += page_text + "\n"

        # Clean up the text
        text_content = " ".join(text_content.split())

        return text_content if text_content.strip() else "‚ùå No text extracted"

    except ImportError:
        return "‚ùå pdfplumber not installed. Use: pip install pdfplumber"
    except Exception as e:
        return f"‚ùå Error extracting PDF: {str(e)}"

# === Step 1: Take user input ===
start_date_str = input("Enter start date (yyyy-mm-dd): ").strip()
try:
    start_date = datetime.strptime(start_date_str, "%Y-%m-%d")
except ValueError:
    print("‚ùå Invalid format. Use yyyy-mm-dd (e.g., 2025-06-01).")
    exit()

today = datetime.today()
delta_days = (today - start_date).days

if delta_days < 0:
    print("‚ùå Start date cannot be in the future.")
    exit()

# === Step 2: Construct the URL ===
url = f"https://www.legislation.gov.au/search/registrationdate(today-{delta_days},today)/collection(act)/sort(registeredat%20desc)"
print(f"\nüîç Fetching data from:\n{url}\n")

# === Step 3: Make request and parse HTML ===
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

response = requests.get(url, headers=headers)
if response.status_code != 200:
    print("‚ùå Failed to fetch the page. Status:", response.status_code)
    exit()

soup = BeautifulSoup(response.text, "html.parser")

# === Step 4: Extract articles ===
results = []
total_items = len(soup.select("frl-grid-cell-title-name-in-force .title-name a"))
print(f"Found {total_items} items to process...\n")

for idx, item in enumerate(soup.select("frl-grid-cell-title-name-in-force .title-name a"), 1):
    print(f"Processing item {idx}/{total_items}...")

    title = item.text.strip()
    relative_link = item.get("href", "").strip()
    base_url = "https://www.legislation.gov.au"
    full_url = base_url + relative_link if relative_link else "N/A"

    # Registered date
    reg_info = item.find_parent().find_next_sibling("div")
    reg_date = "N/A"
    if reg_info and "Registered:" in reg_info.text:
        text = reg_info.get_text(strip=True)
        if "Registered:" in text:
            reg_date = text.split("Registered:")[-1].strip()

    # === Step 4.1: Go inside article page and get effective date ===
    effective_date = "N/A"
    download_link = "N/A"
    content = "N/A"
    act_id = relative_link.split("/")[1] if relative_link else None

    if full_url != "N/A":
        act_text_url = full_url.replace("/latest/text", "/latest/text")
        detail_response = requests.get(act_text_url, headers=headers)

        if detail_response.status_code == 200:
            detail_soup = BeautifulSoup(detail_response.text, "html.parser")
            eff_span = detail_soup.select_one("span.date-effective-start")

            if eff_span:
                effective_date = eff_span.text.strip()

                # Convert to yyyy-mm-dd for URL
                try:
                    eff_dt_obj = datetime.strptime(effective_date, "%d %B %Y")
                    eff_dt_str = eff_dt_obj.strftime("%Y-%m-%d")

                    # Construct download link
                    pdf_url = f"{base_url}/{act_id}/{eff_dt_str}/{eff_dt_str}/text/original/pdf"
                    download_link = f'=HYPERLINK("{pdf_url}", "Download")' # for Excel

                    # === NEW: Extract PDF content from all volumes ===
                    print(f"  Extracting PDF content from: {pdf_url}")
                    content = extract_all_volumes_content(pdf_url, headers)

                    # Limit content length for CSV (optional)
                    if len(content) > 10000:  # Limit to first 10k characters
                        content = content[:10000] + "... [TRUNCATED]"

                except Exception as e:
                    print(f"  ‚ùå Error processing dates: {e}")
                    pass

        time.sleep(1)  # Increased delay to be more polite when downloading PDFs

    results.append({
        "Title": title,
        "Registered Date": reg_date,
        "Effective Date": effective_date,
        "URL": full_url,
        "Download": download_link,
        "Content": content
    })

# === Step 5: Save to DataFrame and CSV ===
df = pd.DataFrame(results)
df.to_csv("acts_scraped_with_content.csv", index=False)
print("\n‚úÖ Done! Saved to 'acts_scraped_with_content.csv'")
print(f"\nProcessed {len(results)} items")
print("\nSample of extracted data:")
print(df[['Title', 'Registered Date', 'Content']].head())
